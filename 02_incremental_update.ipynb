{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 2: 점진적 학습 - 적응적 런타임 업데이트 (CIFAR-10 → SVHN 클래스 추가)\n",
    "\n",
    " CIFAR-10으로 초기 학습을 수행하고, 점진적 학습 단계에서 SVHN의 특정 숫자(7, 8)를 새로운 클래스로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ReplayBuffer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "    def add(self, sample):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(sample)\n",
    "    def get_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CIFAR-10 CNN 모델 (BatchNorm, Dropout, 깊은 구조로 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCIFARCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImprovedCIFARCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    def freeze_features(self):\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    def unfreeze_features(self):\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CIFAR-10 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "cifar_train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "cifar_test_dataset = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_train_dataset, batch_size=128, shuffle=True)\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_test_dataset, batch_size=1000, shuffle=False)\n",
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training on CIFAR-10...\n",
      "Epoch 1: Train Loss: 1.7969, Train Acc: 32.60%, Test Acc: 46.12%\n",
      "Epoch 2: Train Loss: 1.3603, Train Acc: 50.38%, Test Acc: 57.61%\n",
      "Epoch 3: Train Loss: 1.1473, Train Acc: 59.50%, Test Acc: 62.46%\n",
      "Epoch 4: Train Loss: 1.0208, Train Acc: 64.38%, Test Acc: 65.31%\n",
      "Epoch 5: Train Loss: 0.9276, Train Acc: 67.78%, Test Acc: 70.71%\n",
      "Epoch 6: Train Loss: 0.8485, Train Acc: 70.74%, Test Acc: 74.89%\n",
      "Epoch 7: Train Loss: 0.7979, Train Acc: 72.84%, Test Acc: 75.86%\n",
      "Epoch 8: Train Loss: 0.6912, Train Acc: 76.71%, Test Acc: 80.31%\n",
      "Epoch 9: Train Loss: 0.6481, Train Acc: 78.26%, Test Acc: 80.30%\n",
      "Epoch 10: Train Loss: 0.6263, Train Acc: 79.20%, Test Acc: 81.81%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedCIFARCNN(num_classes=10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
    "print('Initial training on CIFAR-10...')\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_epoch(model, cifar_train_loader, optimizer, device)\n",
    "    test_loss, test_acc = test(model, cifar_test_loader, device)\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.classifier output 확장 (10→12) 및 가중치 이전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_classifier(model, num_new_classes=2):\n",
    "    old_linear = model.classifier[-1]\n",
    "    in_features = old_linear.in_features\n",
    "    out_features = old_linear.out_features\n",
    "    new_linear = nn.Linear(in_features, out_features + num_new_classes)\n",
    "    with torch.no_grad():\n",
    "        new_linear.weight[:out_features] = old_linear.weight\n",
    "        new_linear.bias[:out_features] = old_linear.bias\n",
    "    model.classifier[-1] = new_linear\n",
    "    return model\n",
    "\n",
    "model = expand_classifier(model, num_new_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SVHN 7, 8 데이터 로딩 및 점진적 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Selected 10640 training samples and 3679 test samples for digits [7, 8]\n",
      "Performing incremental learning for SVHN digits 7, 8...\n",
      "Epoch 1: Loss: 0.6651, Accuracy: 72.81%\n",
      "Epoch 2: Loss: 0.2789, Accuracy: 88.67%\n",
      "Epoch 3: Loss: 0.2244, Accuracy: 91.17%\n",
      "Epoch 4: Loss: 0.1914, Accuracy: 92.44%\n",
      "Epoch 5: Loss: 0.1799, Accuracy: 93.07%\n"
     ]
    }
   ],
   "source": [
    "svhn_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "svhn_train_dataset = datasets.SVHN('./data', split='train', download=True, transform=svhn_transform)\n",
    "svhn_test_dataset = datasets.SVHN('./data', split='test', transform=svhn_transform)\n",
    "target_digits = [7, 8]\n",
    "svhn_train_indices = [i for i, label in enumerate(svhn_train_dataset.labels) if label in target_digits]\n",
    "svhn_test_indices = [i for i, label in enumerate(svhn_test_dataset.labels) if label in target_digits]\n",
    "svhn_train_subset = torch.utils.data.Subset(svhn_train_dataset, svhn_train_indices)\n",
    "svhn_test_subset = torch.utils.data.Subset(svhn_test_dataset, svhn_test_indices)\n",
    "svhn_train_loader = torch.utils.data.DataLoader(svhn_train_subset, batch_size=128, shuffle=True)\n",
    "svhn_test_loader = torch.utils.data.DataLoader(svhn_test_subset, batch_size=1000, shuffle=False)\n",
    "print(f'Selected {len(svhn_train_subset)} training samples and {len(svhn_test_subset)} test samples for digits {target_digits}')\n",
    "\n",
    "def prepare_svhn_batch(batch):\n",
    "    data, labels = batch\n",
    "    # 7→10, 8→11로 레이블 변환\n",
    "    target = torch.where(labels == 7, torch.tensor(10), labels)\n",
    "    target = torch.where(target == 8, torch.tensor(11), target)\n",
    "    return data, target\n",
    "\n",
    "def incremental_update(model, svhn_loader, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    model.freeze_features()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in svhn_loader:\n",
    "            data, target = prepare_svhn_batch(batch)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "        print(f'Epoch {epoch+1}: Loss: {total_loss/len(svhn_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "    model.unfreeze_features()\n",
    "\n",
    "svhn_optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "print('Performing incremental learning for SVHN digits 7, 8...')\n",
    "incremental_update(model, svhn_train_loader, svhn_optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CIFAR-10, SVHN(7,8) 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on CIFAR-10...\n",
      "CIFAR-10 Test Accuracy: 0.00%\n",
      "Evaluating on SVHN digits 7, 8...\n",
      "SVHN Digit 7 Test Accuracy: 95.64%\n",
      "SVHN Digit 8 Test Accuracy: 96.87%\n"
     ]
    }
   ],
   "source": [
    "def test_svhn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_7 = 0\n",
    "    correct_8 = 0\n",
    "    total_7 = 0\n",
    "    total_8 = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            # 7→10, 8→11\n",
    "            target = torch.where(labels == 7, torch.tensor(10), labels)\n",
    "            target = torch.where(target == 8, torch.tensor(11), target)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_7 += ((labels == 7) & (pred == 10)).sum().item()\n",
    "            correct_8 += ((labels == 8) & (pred == 11)).sum().item()\n",
    "            total_7 += (labels == 7).sum().item()\n",
    "            total_8 += (labels == 8).sum().item()\n",
    "    acc_7 = 100. * correct_7 / total_7 if total_7 > 0 else 0\n",
    "    acc_8 = 100. * correct_8 / total_8 if total_8 > 0 else 0\n",
    "    test_loss /= (total_7 + total_8)\n",
    "    return test_loss, acc_7, acc_8\n",
    "\n",
    "print('Evaluating on CIFAR-10...')\n",
    "cifar_loss, cifar_acc = test(model, cifar_test_loader, device)\n",
    "print(f'CIFAR-10 Test Accuracy: {cifar_acc:.2f}%')\n",
    "print('Evaluating on SVHN digits 7, 8...')\n",
    "svhn_loss, acc_7, acc_8 = test_svhn(model, svhn_test_loader, device)\n",
    "print(f'SVHN Digit 7 Test Accuracy: {acc_7:.2f}%')\n",
    "print(f'SVHN Digit 8 Test Accuracy: {acc_8:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. catastrophic forgetting 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CIFAR-10에서 각 클래스별로 일부 샘플을 뽑아 replay buffer 생성\n",
    "def build_replay_buffer(dataset, n_per_class=200):\n",
    "    # 각 클래스별로 n_per_class개씩 샘플링\n",
    "    class_indices = {i: [] for i in range(10)}\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        if len(class_indices[label]) < n_per_class:\n",
    "            class_indices[label].append(idx)\n",
    "        if all(len(lst) == n_per_class for lst in class_indices.values()):\n",
    "            break\n",
    "    indices = [idx for lst in class_indices.values() for idx in lst]\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "cifar_replay_subset = build_replay_buffer(cifar_train_dataset, n_per_class=100)\n",
    "cifar_replay_loader = torch.utils.data.DataLoader(cifar_replay_subset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_update_with_replay(model, svhn_loader, cifar_replay_loader, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    model.freeze_features()\n",
    "    cifar_iter = iter(cifar_replay_loader)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for svhn_batch in svhn_loader:\n",
    "            # SVHN batch\n",
    "            data, target = prepare_svhn_batch(svhn_batch)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            # CIFAR replay batch (1:1 비율)\n",
    "            try:\n",
    "                cifar_batch = next(cifar_iter)\n",
    "            except StopIteration:\n",
    "                cifar_iter = iter(cifar_replay_loader)\n",
    "                cifar_batch = next(cifar_iter)\n",
    "            data, target = cifar_batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "        print(f'Epoch {epoch+1}: Loss: {total_loss/(2*len(svhn_loader)):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "    model.unfreeze_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing incremental learning for SVHN digits 7, 8...\n",
      "Epoch 1: Loss: 0.5971, Accuracy: 80.47%\n",
      "Epoch 2: Loss: 0.4817, Accuracy: 83.78%\n",
      "Epoch 3: Loss: 0.4281, Accuracy: 85.26%\n",
      "Epoch 4: Loss: 0.3953, Accuracy: 86.22%\n",
      "Epoch 5: Loss: 0.3749, Accuracy: 86.71%\n"
     ]
    }
   ],
   "source": [
    "svhn_optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "print('Performing incremental learning for SVHN digits 7, 8...')\n",
    "incremental_update_with_replay(model, svhn_train_loader, cifar_replay_loader, svhn_optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on CIFAR-10...\n",
      "CIFAR-10 Test Accuracy: 74.61%\n",
      "Evaluating on SVHN digits 7, 8...\n",
      "SVHN Digit 7 Test Accuracy: 88.56%\n",
      "SVHN Digit 8 Test Accuracy: 75.06%\n"
     ]
    }
   ],
   "source": [
    "def test_svhn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_7 = 0\n",
    "    correct_8 = 0\n",
    "    total_7 = 0\n",
    "    total_8 = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            # 7→10, 8→11\n",
    "            target = torch.where(labels == 7, torch.tensor(10), labels)\n",
    "            target = torch.where(target == 8, torch.tensor(11), target)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_7 += ((labels == 7) & (pred == 10)).sum().item()\n",
    "            correct_8 += ((labels == 8) & (pred == 11)).sum().item()\n",
    "            total_7 += (labels == 7).sum().item()\n",
    "            total_8 += (labels == 8).sum().item()\n",
    "    acc_7 = 100. * correct_7 / total_7 if total_7 > 0 else 0\n",
    "    acc_8 = 100. * correct_8 / total_8 if total_8 > 0 else 0\n",
    "    test_loss /= (total_7 + total_8)\n",
    "    return test_loss, acc_7, acc_8\n",
    "\n",
    "print('Evaluating on CIFAR-10...')\n",
    "cifar_loss, cifar_acc = test(model, cifar_test_loader, device)\n",
    "print(f'CIFAR-10 Test Accuracy: {cifar_acc:.2f}%')\n",
    "print('Evaluating on SVHN digits 7, 8...')\n",
    "svhn_loss, acc_7, acc_8 = test_svhn(model, svhn_test_loader, device)\n",
    "print(f'SVHN Digit 7 Test Accuracy: {acc_7:.2f}%')\n",
    "print(f'SVHN Digit 8 Test Accuracy: {acc_8:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
